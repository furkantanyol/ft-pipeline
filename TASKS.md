# TASKS — aitelier

Track progress by checking off tasks as they're completed.

---

## Milestone 1: Core CLI — Init, Add, Rate

### M1.1 — Project Setup

- [x] Initialize monorepo (Turborepo + pnpm)
- [x] Set up `packages/cli` with TypeScript, Commander.js, Vitest
- [x] Configure ESLint, Prettier, tsconfig strict mode
- [x] Set up CI (GitHub Actions: lint + test)
- [x] Create initial README with project description

### M1.2 — `ait init` Command

- [x] Interactive prompts: project name, provider (together/openai), model, system prompt
- [x] Generate `.aitelier.json` config file
- [x] Create `data/` directory with `examples.jsonl`, `train.jsonl`, `val.jsonl`
- [x] Write tests for init flow

### M1.3 — `ait add` Command

- [x] Interactive mode: paste input → paste output → auto-format to chat JSONL
- [x] File mode: `ait add --input input.txt --output output.txt`
- [x] Auto-append to `data/examples.jsonl` with metadata (timestamp, rating: null, version)
- [x] Validate JSON structure on save
- [ ] Support for multi-turn conversations (array of messages)
- [x] Write tests

### M1.4 — `ait rate` Command

- [x] Show unrated examples one by one (interactive)
- [x] Display: system prompt + input + output, formatted in terminal
- [x] Rate 1-10, option to rewrite output inline
- [x] If rewrite: save rewritten version as training target, keep original as metadata
- [x] `ait rate --min 8` — only show examples rated below 8 for re-review
- [x] Summary stats after session
- [x] Write tests

### M1.5 — `ait stats` Command

- [x] Total examples, rated count, unrated count
- [x] Rating distribution (histogram in terminal)
- [x] Examples above quality threshold (configurable, default 8+)
- [x] Train/val split status
- [x] Write tests

### M1.6 — `ait list` Command

- [x] Paginated table view (ID, user preview, assistant preview, rating, split, version)
- [x] Filtering: --rated, --unrated, --min, --max, --split
- [x] Interactive mode: select example → view full details
- [x] Edit messages (user, assistant, system) via editor prompt
- [x] Re-rate examples from detail view
- [x] Delete examples with confirmation
- [x] Add new example from list view
- [x] Non-interactive modes: --json, --no-interactive
- [x] Write tests

---

## Milestone 2: Format, Split, Train

### M2.1 — `ait format` Command

- [x] Export rated examples (above threshold) to provider-specific JSONL
- [x] Together.ai format: standard chat messages
- [x] OpenAI format: standard chat messages with optional `weight` field
- [x] Validate output against provider schema
- [x] Output to `data/train.jsonl` and `data/val.jsonl`
- [x] Write tests

### M2.2 — `ait split` Command

- [x] Auto-split: 80/20 train/val by default
- [x] Stratified by rating if enough examples
- [x] `ait split --ratio 0.9` for custom splits
- [x] Lock validation set: once assigned, val examples don't move to train
- [x] `ait split --reshuffle` to force re-split (with confirmation)
- [x] Write tests

### M2.3 — `ait train` Command

- [x] Upload training file to Together.ai via API
- [x] Create LoRA fine-tune job with sensible defaults
- [x] Configurable: epochs, batch_size, learning_rate, lora_r, lora_alpha
- [x] Save job ID + config to `.aitelier.json` under `runs[]`
- [x] Requires `TOGETHER_API_KEY` env var
- [x] Write tests

### M2.4 — `ait status` Command

- [x] Check fine-tune job status via provider API
- [x] `ait status` — show latest job
- [x] `ait status --all` — show all runs with model IDs
- [x] When complete: save model ID to config
- [x] Write tests

---

## Milestone 3: Eval

### M3.1 — `ait eval` Command

- [x] Run fine-tuned model on all validation examples
- [x] Side-by-side display: expected output vs model output
- [x] Interactive scoring per example
- [x] Summary: average score, sendable rate
- [x] Save eval results to `data/evals/eval-{version}-{date}.json`
- [x] Write tests

### M3.2 — `ait eval --compare` Mode

- [x] Run base model AND fine-tuned model on same inputs
- [x] Show A vs B (blind, randomized order)
- [x] After scoring: reveal which was which + aggregate stats
- [x] Write tests

---

## Milestone 4: README, Examples, Polish

### M4.1 — README.md

- [x] One-line description + quick start
- [ ] CLI demo GIF/video (asciinema or vhs)
- [x] Architecture diagram (mermaid)
- [x] Provider setup guides (Together, OpenAI)
- [x] Example use cases (customer support, code review, domain Q&A)
- [x] Contributing guide
- [x] Comprehensive command documentation with examples
- [x] Troubleshooting section
- [x] Badges and table of contents
- [x] Updated features list and roadmap

### M4.2 — Example Projects

- [x] `examples/customer-support/` — 25 training examples + walkthrough
- [x] `examples/code-review/` — 5 training examples + walkthrough

### M4.3 — Package Publishing

- [x] Publish to npm as `aitelier`
- [x] `npx aitelier` works out of the box
- [x] GitHub releases with changelog
- [x] Badges: npm version, CI status, license

### M4.4 — Launch

- [ ] Hacker News (Show HN)
- [ ] r/LocalLLaMA, r/MachineLearning
- [ ] Twitter/X thread with GIF demo

---

## Web App — Design & UI Guidelines

- **Component library:** Shadcn/UI (new-york style) + Tailwind CSS
- **Design goal:** Elegant, polished, professional — not a developer tool aesthetic. Clean typography, generous whitespace, subtle animations, consistent color palette.
- **Dark mode** as default, with a refined dark palette (not harsh black — use zinc/slate tones)
- Every page should feel intentional and well-crafted, even placeholders

---

## Web App — Milestone 0: Foundation

### W0.1 — Scaffold Next.js 15 App

**Deps:** None · **Size:** S

- [x] Initialize Next.js 15 app (app router) at `packages/web/` with TypeScript, Tailwind CSS, ESLint, `src/` dir
- [x] Install and init Shadcn/UI (new-york style, neutral base color, CSS variables)
- [x] Ensure `pnpm turbo build` builds both CLI and web
- [x] Add `"lint": "eslint src/"` script (Next.js 16 dropped `next lint`)
- [x] Create `packages/web/CLAUDE.md` with web-specific guidelines

### W0.2 — Supabase Client Setup

**Deps:** W0.1 · **Size:** S

- [x] Install `@supabase/supabase-js` and `@supabase/ssr`
- [x] Create server-side client helper (`src/lib/supabase/server.ts`) using `createServerClient`
- [x] Create browser-side client helper (`src/lib/supabase/client.ts`) using `createBrowserClient`
- [x] Create middleware helper (`src/lib/supabase/middleware.ts`) for auth session refresh
- [x] Create placeholder `types.ts` with empty `Database` type
- [x] Add `.env.local.example` with Supabase env var placeholders

### W0.3 — Database Schema & Types

**Deps:** W0.2 · **Size:** M

- [x] Create SQL migration `supabase/migrations/001_initial_schema.sql` with tables: `projects`, `examples`, `training_runs`, `evaluations`, `project_members`
- [x] Add RLS policies (project membership + role-based access: owner/trainer/rater)
- [x] Auto-generate TypeScript types via `pnpm db:gen-types` (uses `supabase gen types`)

### W0.4 — Auth: Magic Link Login

**Deps:** W0.2 · **Size:** M

- [x] Create Next.js middleware (`src/middleware.ts`) for auth session refresh + redirect to `/login`
- [x] Create login page with email input + "Send Magic Link" button (Shadcn Card/Input/Button)
- [x] Create `/auth/callback` route to exchange code for session → redirect to `/dashboard`
- [x] Server action for `signInWithOtp`

### W0.5 — App Shell & Navigation

**Deps:** W0.4 · **Size:** M

- [x] Create `(app)` route group for authenticated pages
- [x] Create sidebar with nav links: Dashboard, Rate, Add, Train, Eval, Playground, Settings
- [x] Active link highlighting, collapsible on mobile
- [x] User menu (email + sign-out)
- [x] Dark mode as default
- [x] Placeholder dashboard page

---

## Web App — Milestone 1: Data Layer

### W1.1 — Project Setup Wizard (Steps 1-3: Basics, Provider, Model)

**Deps:** W0.5, W0.3 · **Size:** L

- [x] Multi-step wizard (client component with step state)
- [x] Step 1 — Basics: project name + description
- [x] Step 2 — Provider: Together.ai selector, API key input + "Test Connection" validation
- [x] Step 3 — Model: dropdown fetched from Together.ai API, highlight recommended models

### W1.2 — Project Setup Wizard (Steps 4-6: Prompt, Config, Invite + Save)

**Deps:** W1.1 · **Size:** M

- [x] Step 4 — System Prompt: textarea with clickable template presets
- [x] Step 5 — Training Config: defaults + expandable advanced section + live cost estimate
- [x] Step 6 — Invite Team: email + role input, skip option
- [x] Save action: insert project + members + send invites → redirect to dashboard

### W1.3 — Add Examples Page

**Deps:** W0.5, W0.3 · **Size:** M

- [x] Manual add tab: input + output textareas, optional rating slider, "Add & Next" flow
- [x] Bulk import tab: JSONL file upload + JSON paste with preview
- [x] Server actions: `addExample`, `importExamples`
- [x] Validation errors for malformed input

### W1.4 — Rating Interface (Card UI + Rating Controls)

**Deps:** W0.5, W0.3 · **Size:** L

- [x] Server component page fetching unrated examples
- [x] RatingCard: input display ("What the AI saw") + output display ("What the AI said")
- [x] Quick buttons: Great (8-10), Okay (5-7), Bad (1-4) with color coding
- [x] Precise 1-10 slider + Skip button
- [x] Progress indicator + session stats
- [x] Auto-advance to next card after rating

### W1.5 — Rating: Rewrite Flow + Keyboard Shortcuts

**Deps:** W1.4 · **Size:** M

- [x] "Rewrite" button → slide-up textarea (Framer Motion) pre-filled with output
- [x] Card transition animations (slide out/in)
- [x] Keyboard shortcuts: `1`-`0` rating, `r` rewrite, `s` skip, `→` next
- [x] Shortcut legend below card

### W1.6 — Rating Filters & Sorting

**Deps:** W1.4 · **Size:** S

- [x] Filter dropdown: Unrated (default), All, Below threshold, Needs rewrite
- [x] Sort dropdown: Newest (default), Random, By rating
- [x] Filters modify Supabase query, persist in session

### W1.7 — Examples Browser Page

**Deps:** W1.3, W1.6 · **Size:** L

- [x] Server actions: `getExamplesList` (paginated, filtered, sorted), `updateExample`, `deleteExample`, `deleteExamples`, `updateExamplesSplit`
- [x] Examples table component: sortable columns, bulk select, row actions, pagination
- [x] Example detail dialog: view/edit mode, inline editing of input/output/rating, delete
- [x] Page with filter/sort controls (All, Rated, Unrated, Quality, Train, Val, Unassigned)
- [x] Sidebar nav item added (Examples between Add and Train)

---

## Web App — Milestone 2: Dashboard & Visualization

### W2.1 — Dashboard: Key Metrics Cards

**Deps:** W0.5, W0.3 · **Size:** M

- [x] Server actions for aggregate stats (total, rated, quality, models trained)
- [x] 4 metric cards with label, number, description
- [x] Skeleton loading state

### W2.2 — Dashboard: Rating Distribution Chart

**Deps:** W2.1 · **Size:** M

- [x] Use ShadCDN Charts, create rating histogram (1-10, color-coded)
- [x] Train/val split stacked bar
- [x] Readiness indicator ("Ready to train" / "Need X more quality examples")

### W2.3 — Dashboard: Training Timeline + Activity Feed

**Deps:** W2.1 · **Size:** M

- [x] Training timeline: horizontal visual of runs (version, count, status badge, clickable)
- [x] Activity feed: last 20 items with relative timestamps
- [ ] Team leaderboard (if multi-user)

---

## Web App — Milestone 3: Training Pipeline

### W3.1 — Training: Pre-flight Check + Config Editor

**Deps:** W0.5, W0.3 · **Size:** M

- [x] Pre-flight: quality count, train split count, cost estimate, duration, warnings, diff from last run
- [x] Config editor: epochs, batch, lr, LoRA params with presets (Conservative/Aggressive)
- [x] Live cost re-estimation on config change

### W3.2 — Training: Split Management

**Deps:** W0.5, W0.3 · **Size:** M

- [x] Visual split editor (unassigned/train/val groups)
- [x] Auto-split button (80/20 stratified)
- [x] Lock/unlock validation set with confirmation
- [x] Split quality indicators (count + avg rating per split)

### W3.3 — Training: Together.ai Integration

**Deps:** W3.1, W3.2 · **Size:** L

- [x] Together.ai provider module: formatExamplesToJSONL, uploadTrainingFile, createFineTuneJob, getJobStatus
- [x] `startTraining` server action: fetch → format → upload → create job → save run
- [x] "Start Training" button with confirmation modal
- [x] Redirect to `/train/[runId]` on launch

### W3.4 — Training: Live Status Page

**Deps:** W3.3 · **Size:** M

- [x] Dynamic route `/train/[runId]` with status display
- [x] Poll Together.ai every 10s, update DB as status changes
- [x] Status badge progression: Uploading → Queued → Training → Complete/Failed
- [x] Cancel button, completed run shows model ID + "Run Evaluation" CTA

### W3.5 — Training: Run History

**Deps:** W3.3 · **Size:** S

- [x] Sortable table of all runs (version, status, model, count, duration, cost, eval score, date)
- [x] Row click → `/train/[runId]`
- [x] Copy model ID button on completed runs

---

## Web App — Milestone 4: Evaluation

### W4.1 — Eval: Setup Page

**Deps:** W0.5, W0.3 · **Size:** S

- [x] Model A / Model B dropdowns (completed runs + baseline)
- [x] Auto-select val set with count display
- [x] "Start Evaluation" → generate outputs for both models → redirect to comparison UI

### W4.2 — Eval: Blind Comparison UI

**Deps:** W4.1 · **Size:** L

- [x] Full-screen layout: input context + two response cards side-by-side
- [x] Random A/B assignment (stored, not re-randomized)
- [x] Preference buttons: "A is better" / "Tie" / "B is better" + optional 1-10 scores
- [x] Keyboard shortcuts: `a`, `t`, `b`
- [x] Progress indicator, save to `evaluations` table

### W4.3 — Eval: Results Reveal + Historical Trends

**Deps:** W4.2 · **Size:** M

- [x] Reveal animation showing model identities
- [x] Win/loss/tie bar (green/red/gray)
- [x] Per-example expandable breakdown
- [x] Verdict text ("SHIP IT" / "need more data")
- [x] Historical trends line chart (ShadCDN Charts)

---

## Web App — Milestone 5: Playground

### W5.1 — Playground: Single Model Chat

**Deps:** W0.5, W0.3 · **Size:** M

- [x] Input textarea + streaming output panel
- [x] Model selector (completed runs + base model)
- [x] Temperature + max tokens sliders
- [x] Generate / Regenerate buttons
- [x] "Save as Example" button → creates unrated example

### W5.2 — Playground: Side-by-Side Comparison

**Deps:** W5.1 · **Size:** S

- [x] "Compare" toggle → two output panels with independent model selectors
- [x] Same input → both models stream simultaneously
- [x] Quick preference buttons + response time display

---

## Web App — Milestone 6: Settings & Team

### W6.1 — Settings Page

**Deps:** W0.5, W0.3 · **Size:** M

- [x] Project settings: name, description
- [x] Provider config: API key, model, "Test Connection"
- [x] Training defaults: epochs, batch, lr, LoRA params
- [x] Team management: list members, invite, change role, remove
- [x] Export: "Download Dataset as JSONL"
- [x] Danger zone: "Delete Project" with double confirmation

---

## Web App — Milestone 7: Polish

### W7.1 — Mobile-Responsive Rating + Animations

**Deps:** W1.5 · **Size:** M

- [ ] Full mobile-responsive rating interface (full-width cards, stacked buttons, 44px tap targets)
- [ ] Swipe gestures: right = good (8), left = bad (3) with tilt + color overlay
- [ ] Polish card transition animations
- [ ] Light/dark theme and switch in settings

### W7.2 — Real-time Collaboration

**Deps:** W0.2 · **Size:** M

- [ ] `useRealtime` hook for Supabase Realtime subscriptions
- [ ] Dashboard: auto-update metrics on data changes
- [ ] Rating page: skip examples rated by others + toast notification
- [ ] Team activity toasts ("David just rated 3 examples")

### W7.3 — Empty States & Onboarding

**Deps:** W0.5 · **Size:** S

- [x] Reusable `EmptyState` component (icon, title, description, action button)
- [x] Empty states for all pages guiding to the correct next action
- [x] "What's next?" contextual suggestion on dashboard based on project state

---

## Web Task Dependency Graph

```
W0.1 (Scaffold)
 ├── W0.2 (Supabase Client)
 │    ├── W0.3 (DB Schema + Types)
 │    │    ├── W1.1 → W1.2 (Setup Wizard)
 │    │    ├── W1.3 (Add Examples)
 │    │    ├── W1.4 → W1.5 (Rating + Rewrite)
 │    │    │         └── W1.6 (Rating Filters)
 │    │    │         └── W7.1 (Mobile Rating)
 │    │    ├── W2.1 → W2.2 (Dashboard Charts)
 │    │    │    └── W2.3 (Timeline + Feed)
 │    │    ├── W3.1 (Preflight + Config)
 │    │    │    └── W3.2 (Split Manager)
 │    │    │         └── W3.3 → W3.4 (Together.ai + Status)
 │    │    │              └── W3.5 (Run History)
 │    │    ├── W4.1 → W4.2 → W4.3 (Eval Pipeline)
 │    │    ├── W5.1 → W5.2 (Playground)
 │    │    └── W6.1 (Settings)
 │    └── W7.2 (Realtime)
 └── W0.4 (Auth)
      └── W0.5 (App Shell)
           └── (all page tasks)

W7.3 (Empty States) depends on W0.5
```
